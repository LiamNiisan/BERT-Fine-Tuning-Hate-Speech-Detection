{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24000b07-2991-4ee4-8cb1-10ce6e2e5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from src.data_collection import get_data\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb52a14-08fa-4189-ae85-634e0ac3a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-f91f636a830ad73c\n",
      "Reusing dataset parquet (C:\\Users\\UTKARSH\\.cache\\huggingface\\datasets\\parquet\\ucberkeley-dlab--measuring-hate-speech-f91f636a830ad73c\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ad9a4f31ae42cf83a5f7aa9d5beb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "hate_speech_ucb = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b9855c-f96f-4f87-97ac-55ca0d44ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14a1e8b-1c67-4022-904f-42d44ba99c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"  # \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "TOKENIZER = DistilBertTokenizer.from_pretrained(MODEL_NAME, truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e54fc0d-33b8-4f11-91dc-49eb74d98993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39565 entries, 0 to 39564\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text        39565 non-null  object\n",
      " 1   hatespeech  39565 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 347.9+ KB\n"
     ]
    }
   ],
   "source": [
    "hate_speech_ucb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d6e35e-0924-487e-9cb7-bc12d57c9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26608\n",
       "1    12957\n",
       "Name: hatespeech, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_ucb[\"hatespeech\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51955c44-f215-4c16-9eb9-c97292c757fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing purpose\n",
    "# hate_speech_ucb = hate_speech_ucb[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0617aa9-d2e3-4956-a51f-5815c09b0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! thank u! im transmasc and generally present masc i Maybe look a bit gay so i was wondering about how that would go. im sorry u were heckled but its good to hear things are better there now. tysm my favorite lesbian :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Go fuck yourself faggot!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!flair [I love women and minorities]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!flair [death to all niggers and gays]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" 'convoluted' genealogy of Jesus\"; was that comment really necessary? I am sure that you have regular Christian viewers--like myself--who might not have appreciated that very much.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39560</th>\n",
       "      <td>ðŸ¤§  The limousine liberal Jews and WASPs who want to assuage their white guilt.. SJW maladjusted gays and nigs so they can pester and bother me; bullshit wack monotheist Judeo-Christianity values that's I'm suck when I leave Chinatown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39561</th>\n",
       "      <td>ðŸ¤¬CONGRATULATIONS #MARYLAND #BALTIMORE LIBERALS &amp; OBAMA... YOU REACHED A NEW LOW If I didn't know better, I would have thought this was Europe. Once again, Democrat Mayor Police Video Shows Whites Being Targeted During Memorial Day Chaos in Baltimore URL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39562</th>\n",
       "      <td>ðŸ¥³ another bitch will when another bitch can't</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39563</th>\n",
       "      <td>ðŸ¥´ðŸ¥´ next time stay your ass in the car before our \"beaner\" asses beat yah URL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39564</th>\n",
       "      <td>ðŸ§¬ðŸ§¬ðŸ§¬ *IM SMARTER THAN SSSNIPERWOLF.I KNOW EVERYTHING ABOUT RACE AND COLOR SCIENCE. SSSNIPERWOLF WANTS SWEDES TO FREEZE AND BLACKS TO BURN. My videos are the only TRUTH ABOUT RACE WHITE AND BLACK. IM WHITE AND I KNOW EVERYTHING ABOUT BLACK PEOPLE* HI SNIPERWOLF I LIVE IN GLENDALE TOO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39565 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             text  \\\n",
       "0                                                                     ! thank u! im transmasc and generally present masc i Maybe look a bit gay so i was wondering about how that would go. im sorry u were heckled but its good to hear things are better there now. tysm my favorite lesbian :)   \n",
       "1                                                                                                                                                                                                                                                                       !Go fuck yourself faggot!   \n",
       "2                                                                                                                                                                                                                                                            !flair [I love women and minorities]   \n",
       "3                                                                                                                                                                                                                                                          !flair [death to all niggers and gays]   \n",
       "4                                                                                                           \" 'convoluted' genealogy of Jesus\"; was that comment really necessary? I am sure that you have regular Christian viewers--like myself--who might not have appreciated that very much.   \n",
       "...                                                                                                                                                                                                                                                                                           ...   \n",
       "39560                                                   ðŸ¤§  The limousine liberal Jews and WASPs who want to assuage their white guilt.. SJW maladjusted gays and nigs so they can pester and bother me; bullshit wack monotheist Judeo-Christianity values that's I'm suck when I leave Chinatown   \n",
       "39561                               ðŸ¤¬CONGRATULATIONS #MARYLAND #BALTIMORE LIBERALS & OBAMA... YOU REACHED A NEW LOW If I didn't know better, I would have thought this was Europe. Once again, Democrat Mayor Police Video Shows Whites Being Targeted During Memorial Day Chaos in Baltimore URL   \n",
       "39562                                                                                                                                                                                                                                               ðŸ¥³ another bitch will when another bitch can't   \n",
       "39563                                                                                                                                                                                                                ðŸ¥´ðŸ¥´ next time stay your ass in the car before our \"beaner\" asses beat yah URL   \n",
       "39564  ðŸ§¬ðŸ§¬ðŸ§¬ *IM SMARTER THAN SSSNIPERWOLF.I KNOW EVERYTHING ABOUT RACE AND COLOR SCIENCE. SSSNIPERWOLF WANTS SWEDES TO FREEZE AND BLACKS TO BURN. My videos are the only TRUTH ABOUT RACE WHITE AND BLACK. IM WHITE AND I KNOW EVERYTHING ABOUT BLACK PEOPLE* HI SNIPERWOLF I LIVE IN GLENDALE TOO   \n",
       "\n",
       "       hatespeech  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  \n",
       "...           ...  \n",
       "39560           1  \n",
       "39561           0  \n",
       "39562           1  \n",
       "39563           1  \n",
       "39564           0  \n",
       "\n",
       "[39565 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_ucb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6597b126-71c3-4bf6-a4e0-54216248c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = OneHotEncoder(sparse=False).fit_transform(np.array(self.data[\"hatespeech\"]).reshape(-1, 1))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7e0254-57a2-431b-926d-e00baa0a20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (39565, 2)\n",
      "TRAIN Dataset: (31652, 2)\n",
      "VAL Dataset: (3957, 3)\n",
      "TEST Dataset: (3956, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "\n",
    "train_data = hate_speech_ucb.sample(frac=train_size, random_state=210)\n",
    "test_data = hate_speech_ucb.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = test_data.sample(frac=val_size / (1 - train_size), random_state=220).reset_index()\n",
    "test_data = test_data.drop(val_data.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {hate_speech_ucb.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_data.shape}\")\n",
    "print(f\"VAL Dataset: {val_data.shape}\")\n",
    "print(f\"TEST Dataset: {test_data.shape}\")\n",
    "\n",
    "training_set = HateDataset(train_data, TOKENIZER, MAX_LEN)\n",
    "validation_set = HateDataset(val_data, TOKENIZER, MAX_LEN)\n",
    "testing_set = HateDataset(test_data, TOKENIZER, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fd0ff6-f04d-4cac-8a49-af7089ae19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    \"batch_size\": 1,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    \"batch_size\": 1,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49eff14-562f-4c58-8b61-5434b44f92e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTMultiClass(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DistilBERTMultiClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d28ec9-faf4-43ff-ad17-e486f6933f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = hate_speech_ucb[\"hatespeech\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce980064-cc77-4f8a-83c1-bd000bcef26d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTMultiClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBERTMultiClass(n_classes=N_CLASSES)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e17ea5e-1dec-43d2-bd11-5a7d80fad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132df9ce-366f-4aed-aaf6-519f21c3018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69af06e-1a83-40e8-88c6-8a2a8d6552ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _ % 1000 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9105846-c745-4002-887d-7120812fce14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.693435549736023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.48804569244384766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:54,  3.33it/s]\n",
      "1it [00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3577663004398346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:15,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.16716288030147552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [10:51,  3.04it/s]\n",
      "1it [00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.38692086935043335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:53,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.3680565357208252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [10:59,  3.00it/s]\n",
      "1it [00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.3465307950973511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.11053898930549622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:54,  3.33it/s]\n",
      "1it [00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.2167648822069168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:59,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.15157343447208405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:52,  3.34it/s]\n",
      "1it [00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.13486319780349731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:59,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.023350607603788376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:52,  3.34it/s]\n",
      "1it [00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.006674409843981266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.0315849743783474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:53,  3.34it/s]\n",
      "1it [00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.022003773599863052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.014893965795636177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:54,  3.33it/s]\n",
      "1it [00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.017017576843500137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:01,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.006195249035954475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:55,  3.33it/s]\n",
      "1it [00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.018526898697018623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.015277309343218803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [09:53,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a62ecd-d64b-46a6-8913-4c9f14df2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(loader, 0)):\n",
    "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e08ffa6-b41d-44c1-91fa-c99d961d86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3957it [00:48, 81.94it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(model, validation_loader)\n",
    "\n",
    "final_outputs = np.argmax(outputs, axis=1)\n",
    "targets = np.argmax(targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36f85c86-374e-430e-ba42-e99448e0221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3033 / 3957 correct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Got {sum(final_outputs == targets)} / {len(final_outputs)} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18486d67-6d06-4670-b895-4680a8ebd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 score:\t\t0.766\n",
      "Macro F1 score:\t\t0.728\n",
      "Weighted F1 score:\t0.763\n"
     ]
    }
   ],
   "source": [
    "micro_f1 = f1_score(targets, final_outputs, average=\"micro\")\n",
    "macro_f1 = f1_score(targets, final_outputs, average=\"macro\")\n",
    "weighted_f1 = f1_score(targets, final_outputs, average=\"weighted\")\n",
    "\n",
    "print(f\"Micro F1 score:\\t\\t{round(micro_f1, 3)}\")\n",
    "print(f\"Macro F1 score:\\t\\t{round(macro_f1, 3)}\")\n",
    "print(f\"Weighted F1 score:\\t{round(weighted_f1, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ffe1802-2337-4911-a317-5f6301975dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      2665\n",
      "           1       0.66      0.60      0.62      1292\n",
      "\n",
      "    accuracy                           0.77      3957\n",
      "   macro avg       0.73      0.72      0.73      3957\n",
      "weighted avg       0.76      0.77      0.76      3957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets, final_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5e411ac-7ed9-407c-a3d6-144e99320f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "output_model_file = \"../models/pytorch_distilbert.bin\"\n",
    "output_vocab_file = \"../models/vocab_distilbert.bin\"\n",
    "\n",
    "torch.save(model, output_model_file)\n",
    "TOKENIZER.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
